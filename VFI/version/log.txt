

{230622 GT Itp Flow로 기대성능 평가를 위한 모델 학습 시뮬레이션}
{Train Dataloader Stride 2 주고, UNet 구조 간소화 수행}
python train.py --data_root /home/work/VFIT/data/vimeo_septuplet/ --mode train --log_iter 1000 --epochs 2 --batch_size 6 --test_batch_size 4 --checkpoint_epoch 1

>>>>>> RAFT + MiDaS Guide Video Frame Interpolation <<<<<

Config: Namespace(batch_size=6, checkpoint_epoch=1, checkpoint_root='./checkpoint/', crop_size=(512, 960),
cuda=True, data_root='/home/work/VFIT/data/vimeo_septuplet/', depth_model='DPT_Large', device='cuda', epochs=2,
flow_model='raft_large', gpu=0, load_from='checkpoint/model_best.pth', log_iter=1000, loss='1*l1+0.5*l2+0.5*Huber',
mode='train', num_workers=4, out_root='./output/temp/', seed=1123, tensorboard_root='./tensorboard/', test_batch_size=4, use_gpu=True)

>>>>>>>>>>>>>>>>>>>>> Initialize <<<<<<<<<<<<<<<<<<<<<<<<
train RAFT + MiDaS Guide VFI DataLoader: 145818 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.407
valid RAFT + MiDaS Guide VFI DataLoader: 18518 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([4, 3, 512, 960]),  gt: torch.Size([4, 3, 512, 960]),  torch.float32,  0.482
CUDA available True,  Usuable devices: 1,   Current device: 0,  name: NVIDIA A100-PCIE-40GB

raft_large #params 5257536
DPT_Large #params 344055465
Refine Net #params 7990595 

1.000 * l1
0.500 * l2
0.500 * Huber

>>>>>>>>>>>>>>>>>>> Train & Valid <<<<<<<<<<<<<<<<<<<<<<
2023-06-23 02:44:32 Train Epoch: 1 [1/24303]    Loss: 0.6657    PSNR: 7.2066  Lr:0.0040
2023-06-23 03:13:07 Train Epoch: 1 [1001/24303] Loss: 109.7511  PSNR: 19.1958  Lr:0.0040
2023-06-23 03:41:42 Train Epoch: 1 [2001/24303] Loss: 0.0823    PSNR: 19.0492  Lr:0.0040
2023-06-23 04:10:17 Train Epoch: 1 [3001/24303] Loss: 0.0547    PSNR: 25.6064  Lr:0.0040
2023-06-23 04:38:52 Train Epoch: 1 [4001/24303] Loss: 0.0394    PSNR: 24.8786  Lr:0.0040
2023-06-23 05:07:29 Train Epoch: 1 [5001/24303] Loss: 0.0308    PSNR: 24.8678  Lr:0.0040
2023-06-23 05:36:06 Train Epoch: 1 [6001/24303] Loss: 0.0271    PSNR: 28.0967  Lr:0.0040
syn: 0.29155102372169495, out: 0.2956215739250183, gt: 0.2891116738319397
syn: 0.2957470417022705, out: 0.29347220063209534, gt: 0.29765409231185913
syn: 0.339611291885376, out: 0.33498865365982056, gt: 0.33860719203948975
syn: 0.2827676236629486, out: 0.27897247672080994, gt: 0.28141072392463684
syn: 0.35691410303115845, out: 0.3553059697151184, gt: 0.35641995072364807
syn: 0.3405797481536865, out: 0.3452586829662323, gt: 0.3390345871448517
syn: 0.41265353560447693, out: 0.41578781604766846, gt: 0.41385021805763245
syn: 0.24283559620380402, out: 0.23735882341861725, gt: 0.24469485878944397
syn: 0.3406248688697815, out: 0.33183252811431885, gt: 0.3452826142311096
2023-06-23 06:28:10 Train Epoch: 1 [7001/24303] Loss: 0.0237    PSNR: 30.0999  Lr:0.0040
2023-06-23 06:56:49 Train Epoch: 1 [8001/24303] Loss: 0.0481    PSNR: 29.8229  Lr:0.0040
2023-06-23 07:25:27 Train Epoch: 1 [9001/24303] Loss: 0.0200    PSNR: 25.1766  Lr:0.0040
2023-06-23 07:54:06 Train Epoch: 1 [10001/24303]        Loss: 0.0199    PSNR: 26.6583  Lr:0.0040
2023-06-23 08:22:45 Train Epoch: 1 [11001/24303]        Loss: 0.0209    PSNR: 26.7764  Lr:0.0040
2023-06-23 08:51:24 Train Epoch: 1 [12001/24303]        Loss: 0.0202    PSNR: 29.7657  Lr:0.0040
2023-06-23 09:20:02 Train Epoch: 1 [13001/24303]        Loss: 0.0220    PSNR: 30.6524  Lr:0.0040
2023-06-23 09:48:40 Train Epoch: 1 [14001/24303]        Loss: 0.0207    PSNR: 27.9566  Lr:0.0040
2023-06-23 10:17:17 Train Epoch: 1 [15001/24303]        Loss: 389.0121  PSNR: 26.3210  Lr:0.0040
2023-06-23 10:45:53 Train Epoch: 1 [16001/24303]        Loss: 0.0194    PSNR: 29.8594  Lr:0.0040
2023-06-23 11:14:30 Train Epoch: 1 [17001/24303]        Loss: 0.0179    PSNR: 30.1149  Lr:0.0040
2023-06-23 11:43:07 Train Epoch: 1 [18001/24303]        Loss: 0.0172    PSNR: 29.2919  Lr:0.0040
2023-06-23 12:11:43 Train Epoch: 1 [19001/24303]        Loss: 0.0171    PSNR: 29.9263  Lr:0.0040
2023-06-23 12:40:20 Train Epoch: 1 [20001/24303]        Loss: 0.0169    PSNR: 28.4608  Lr:0.0040
2023-06-23 13:08:56 Train Epoch: 1 [21001/24303]        Loss: 0.0165    PSNR: 31.0322  Lr:0.0040
2023-06-23 13:37:33 Train Epoch: 1 [22001/24303]        Loss: 0.0171    PSNR: 25.5377  Lr:0.0040
2023-06-23 14:06:10 Train Epoch: 1 [23001/24303]        Loss: 0.0166    PSNR: 27.1924  Lr:0.0040
2023-06-23 14:34:48 Train Epoch: 1 [24001/24303]        Loss: 0.0168    PSNR: 29.8705  Lr:0.0040
Validation for epoch = 1
(48325s) Epoch [1/5], Val_PSNR:29.95, Val_SSIM:0.9999
2023-06-23 16:09:38 Train Epoch: 2 [1/24303]    Loss: 0.0164    PSNR: 28.2476  Lr:0.0040
2023-06-23 16:38:11 Train Epoch: 2 [1001/24303] Loss: 0.0172    PSNR: 29.8472  Lr:0.0040
2023-06-23 17:06:43 Train Epoch: 2 [2001/24303] Loss: 0.0169    PSNR: 29.8877  Lr:0.0040
2023-06-23 17:35:15 Train Epoch: 2 [3001/24303] Loss: 0.0175    PSNR: 30.0147  Lr:0.0040
2023-06-23 18:03:47 Train Epoch: 2 [4001/24303] Loss: 2.9257    PSNR: 25.4779  Lr:0.0040
2023-06-23 18:32:20 Train Epoch: 2 [5001/24303] Loss: 0.0294    PSNR: 28.9213  Lr:0.0040
2023-06-23 19:00:53 Train Epoch: 2 [6001/24303] Loss: 0.0205    PSNR: 30.0448  Lr:0.0040
syn: 0.2886967360973358, out: 0.2920430302619934, gt: 0.29096120595932007
syn: 0.3693748414516449, out: 0.37403929233551025, gt: 0.37426385283470154
syn: 0.32181093096733093, out: 0.3231981098651886, gt: 0.32043954730033875
syn: 0.3025142550468445, out: 0.2989168167114258, gt: 0.3046593964099884
syn: 0.31054845452308655, out: 0.30548495054244995, gt: 0.3118654489517212
syn: 0.30176877975463867, out: 0.29817432165145874, gt: 0.30218979716300964
syn: 0.29482442140579224, out: 0.2951619327068329, gt: 0.2959008514881134
syn: 0.4445956349372864, out: 0.4515382647514343, gt: 0.44476601481437683
syn: 0.4538138508796692, out: 0.45777371525764465, gt: 0.4559187591075897
2023-06-23 20:39:33 Train Epoch: 2 [7001/24303] Loss: 0.0203    PSNR: 29.1145  Lr:0.0040
2023-06-23 21:08:07 Train Epoch: 2 [8001/24303] Loss: 0.0402    PSNR: 30.4357  Lr:0.0040
2023-06-23 21:36:41 Train Epoch: 2 [9001/24303] Loss: 0.0180    PSNR: 32.1160  Lr:0.0040
2023-06-23 22:05:13 Train Epoch: 2 [10001/24303]        Loss: 0.0181    PSNR: 27.5659  Lr:0.0040
2023-06-23 22:33:45 Train Epoch: 2 [11001/24303]        Loss: 0.0180    PSNR: 30.0003  Lr:0.0040
2023-06-23 23:02:18 Train Epoch: 2 [12001/24303]        Loss: 0.0178    PSNR: 27.4789  Lr:0.0040
2023-06-23 23:30:53 Train Epoch: 2 [13001/24303]        Loss: 842322.2017       PSNR: -2.3600  Lr:0.0040
2023-06-23 23:59:29 Train Epoch: 2 [14001/24303]        Loss: 1.9433    PSNR: 1.4054  Lr:0.0040
2023-06-24 00:28:06 Train Epoch: 2 [15001/24303]        Loss: 0.6034    PSNR: 8.1831  Lr:0.0040
2023-06-24 00:56:42 Train Epoch: 2 [16001/24303]        Loss: 0.3817    PSNR: 6.6203  Lr:0.0040
2023-06-24 01:25:17 Train Epoch: 2 [17001/24303]        Loss: 0.3176    PSNR: 14.8615  Lr:0.0040
2023-06-24 01:53:53 Train Epoch: 2 [18001/24303]        Loss: 0.1702    PSNR: 20.5973  Lr:0.0040
2023-06-24 02:22:28 Train Epoch: 2 [19001/24303]        Loss: 0.0704    PSNR: 22.6415  Lr:0.0040
2023-06-24 02:51:04 Train Epoch: 2 [20001/24303]        Loss: 0.0319    PSNR: 24.5069  Lr:0.0040
2023-06-24 03:19:41 Train Epoch: 2 [21001/24303]        Loss: 0.0247    PSNR: 30.3628  Lr:0.0040
2023-06-24 03:48:18 Train Epoch: 2 [22001/24303]        Loss: 0.0248    PSNR: 33.0902  Lr:0.0040
2023-06-24 04:16:57 Train Epoch: 2 [23001/24303]        Loss: 8.6767    PSNR: -17.9804  Lr:0.0040
2023-06-24 04:45:33 Train Epoch: 2 [24001/24303]        Loss: 0.4394    PSNR: 21.8295  Lr:0.0040
Validation for epoch = 2
(51014s) Epoch [2/5], Val_PSNR:23.05, Val_SSIM:0.9996
2023-06-24 06:19:51 Train Epoch: 3 [1/24303]    Loss: 0.0490    PSNR: 22.1119  Lr:0.0040
2023-06-24 06:48:26 Train Epoch: 3 [1001/24303] Loss: 0.0349    PSNR: 27.4080  Lr:0.0040
2023-06-24 07:17:00 Train Epoch: 3 [2001/24303] Loss: 0.0329    PSNR: 26.9708  Lr:0.0040
2023-06-24 07:45:35 Train Epoch: 3 [3001/24303] Loss: 0.0217    PSNR: 29.5575  Lr:0.0040
2023-06-24 08:14:10 Train Epoch: 3 [4001/24303] Loss: 0.0222    PSNR: 27.6235  Lr:0.0040
2023-06-24 08:42:45 Train Epoch: 3 [5001/24303] Loss: 72788.3566        PSNR: -37.4642  Lr:0.0040
2023-06-24 09:11:20 Train Epoch: 3 [6001/24303] Loss: 256.9548  PSNR: -10.9433  Lr:0.0040
syn: 0.3392910361289978, out: -0.7594705820083618, gt: 0.3408051133155823
syn: 0.24066981673240662, out: -0.6398389935493469, gt: 0.24058854579925537
syn: 0.3553893268108368, out: -0.8749550580978394, gt: 0.3551667332649231
syn: 0.32241424918174744, out: -0.6930815577507019, gt: 0.32215416431427
syn: 0.24678285419940948, out: -0.46510037779808044, gt: 0.2482827603816986
syn: 0.42435353994369507, out: -0.7619943022727966, gt: 0.4282386004924774
syn: 0.33605650067329407, out: -0.7095354199409485, gt: 0.33738189935684204
syn: 0.3630465567111969, out: -0.7726624608039856, gt: 0.3663748800754547
syn: 0.3710077404975891, out: -0.8017834424972534, gt: 0.37147125601768494
2023-06-24 11:37:27 Train Epoch: 3 [7001/24303] Loss: 7.0225    PSNR: -8.9054  Lr:0.0040
2023-06-24 12:06:00 Train Epoch: 3 [8001/24303] Loss: 9.0240    PSNR: -2.6519  Lr:0.0040
2023-06-24 12:34:34 Train Epoch: 3 [9001/24303] Loss: 24.6885   PSNR: -3.2113  Lr:0.0040
2023-06-24 13:03:06 Train Epoch: 3 [10001/24303]        Loss: 1.6037    PSNR: 1.2171  Lr:0.0040
2023-06-24 13:31:36 Train Epoch: 3 [11001/24303]        Loss: 2.0660    PSNR: 2.4313  Lr:0.0040
2023-06-24 14:00:06 Train Epoch: 3 [12001/24303]        Loss: 237855.1182       PSNR: -27.4905  Lr:0.0040
2023-06-24 14:28:39 Train Epoch: 3 [13001/24303]        Loss: 57.2295   PSNR: -11.5659  Lr:0.0040
2023-06-24 14:57:13 Train Epoch: 3 [14001/24303]        Loss: 19.0006   PSNR: -9.8795  Lr:0.0040
2023-06-24 15:25:47 Train Epoch: 3 [15001/24303]        Loss: 7.3467    PSNR: -5.6624  Lr:0.0040
2023-06-24 15:54:21 Train Epoch: 3 [16001/24303]        Loss: 3.0325    PSNR: -2.1971  Lr:0.0040
2023-06-24 16:22:55 Train Epoch: 3 [17001/24303]        Loss: 974652.6510       PSNR: -22.5323  Lr:0.0040
2023-06-24 16:51:27 Train Epoch: 3 [18001/24303]        Loss: 70.3654   PSNR: -18.2052  Lr:0.0040
2023-06-24 17:20:00 Train Epoch: 3 [19001/24303]        Loss: 69.1813   PSNR: -15.1153  Lr:0.0040
2023-06-24 17:48:32 Train Epoch: 3 [20001/24303]        Loss: 9.2968    PSNR: -9.6896  Lr:0.0040
2023-06-24 18:17:04 Train Epoch: 3 [21001/24303]        Loss: 7.1883    PSNR: -5.4614  Lr:0.0040
2023-06-24 18:45:38 Train Epoch: 3 [22001/24303]        Loss: 8.1756    PSNR: -1.5030  Lr:0.0040
2023-06-24 19:14:11 Train Epoch: 3 [23001/24303]        Loss: 83463.0720        PSNR: -11.8902  Lr:0.0040
2023-06-24 19:42:44 Train Epoch: 3 [24001/24303]        Loss: 11.6236   PSNR: -5.8444  Lr:0.0040
Validation for epoch = 3
(53835s) Epoch [3/5], Val_PSNR:-7.68, Val_SSIM:0.9729
2023-06-24 21:17:06 Train Epoch: 4 [1/24303]    Loss: 6.0313    PSNR: -9.1098  Lr:0.0040
2023-06-24 21:45:39 Train Epoch: 4 [1001/24303] Loss: 4.0420    PSNR: -4.0941  Lr:0.0040
2023-06-24 22:14:12 Train Epoch: 4 [2001/24303] Loss: 2.4903    PSNR: -2.7472  Lr:0.0040
2023-06-24 22:42:47 Train Epoch: 4 [3001/24303] Loss: 1.8015    PSNR: 1.1150  Lr:0.0040
2023-06-24 23:11:21 Train Epoch: 4 [4001/24303] Loss: 1.7431    PSNR: 5.4808  Lr:0.0040
2023-06-24 23:39:56 Train Epoch: 4 [5001/24303] Loss: 0.4576    PSNR: 7.9801  Lr:0.0040
2023-06-25 00:08:27 Train Epoch: 4 [6001/24303] Loss: 0.3718    PSNR: 14.1800  Lr:0.0040
syn: 0.3057211637496948, out: 0.2514467239379883, gt: 0.30685052275657654
syn: 0.3235613703727722, out: 0.2746645212173462, gt: 0.32876983284950256
syn: 0.3388851284980774, out: 0.3557378053665161, gt: 0.3402869999408722
syn: 0.33462050557136536, out: 0.330499529838562, gt: 0.3372107446193695
syn: 0.2931540906429291, out: 0.1486666351556778, gt: 0.29442745447158813
syn: 0.3807277977466583, out: 0.1719469577074051, gt: 0.3802497088909149
syn: 0.3824118971824646, out: 0.1979035586118698, gt: 0.3814936876296997
syn: 0.4344654679298401, out: 0.2957346439361572, gt: 0.43464311957359314
syn: 0.401096373796463, out: 0.3837975263595581, gt: 0.401686429977417
2023-06-25 03:20:53 Train Epoch: 4 [7001/24303] Loss: 0.3651    PSNR: 17.9837  Lr:0.0040
2023-06-25 03:49:27 Train Epoch: 4 [8001/24303] Loss: 0.0733    PSNR: 23.8146  Lr:0.0040
2023-06-25 04:17:59 Train Epoch: 4 [9001/24303] Loss: 0.0389    PSNR: 30.0377  Lr:0.0040
2023-06-25 04:46:33 Train Epoch: 4 [10001/24303]        Loss: 599150886.6935    PSNR: -41.2850  Lr:0.0040
2023-06-25 05:15:08 Train Epoch: 4 [11001/24303]        Loss: 2051.6683 PSNR: -29.9526  Lr:0.0040
2023-06-25 05:43:43 Train Epoch: 4 [12001/24303]        Loss: 235.2783  PSNR: -20.4095  Lr:0.0040
2023-06-25 06:12:18 Train Epoch: 4 [13001/24303]        Loss: 283.7377  PSNR: -13.3479  Lr:0.0040
2023-06-25 06:40:53 Train Epoch: 4 [14001/24303]        Loss: 24.9926   PSNR: -13.7535  Lr:0.0040
2023-06-25 07:09:28 Train Epoch: 4 [15001/24303]        Loss: 166.7055  PSNR: -6.7310  Lr:0.0040
2023-06-25 07:38:02 Train Epoch: 4 [16001/24303]        Loss: 430.1545  PSNR: -12.1389  Lr:0.0040
2023-06-25 08:06:36 Train Epoch: 4 [17001/24303]        Loss: 7.2811    PSNR: -5.8524  Lr:0.0040
2023-06-25 08:35:10 Train Epoch: 4 [18001/24303]        Loss: 4.2148    PSNR: -3.0971  Lr:0.0040
2023-06-25 09:03:44 Train Epoch: 4 [19001/24303]        Loss: 26.5954   PSNR: -4.0506  Lr:0.0040
2023-06-25 09:32:16 Train Epoch: 4 [20001/24303]        Loss: 3.4479    PSNR: 3.0840  Lr:0.0040
2023-06-25 10:00:49 Train Epoch: 4 [21001/24303]        Loss: 0.7629    PSNR: 3.9885  Lr:0.0040
2023-06-25 10:29:21 Train Epoch: 4 [22001/24303]        Loss: 1.1071    PSNR: 0.8714  Lr:0.0040
2023-06-25 10:57:56 Train Epoch: 4 [23001/24303]        Loss: 19965820461.3239  PSNR: -73.0731  Lr:0.0040
2023-06-25 11:26:30 Train Epoch: 4 [24001/24303]        Loss: 7506279.6837      PSNR: -66.0500  Lr:0.0040
Validation for epoch = 4
(56629s) Epoch [4/5], Val_PSNR:-63.63, Val_SSIM:0.9714
2023-06-25 13:00:55 Train Epoch: 5 [1/24303]    Loss: 1798379.3750      PSNR: -63.6006  Lr:0.0040
2023-06-25 13:29:26 Train Epoch: 5 [1001/24303] Loss: 1059987.1555      PSNR: -57.3215  Lr:0.0040
2023-06-25 13:57:56 Train Epoch: 5 [2001/24303] Loss: 392129.4819       PSNR: -56.8478  Lr:0.0040
2023-06-25 14:26:28 Train Epoch: 5 [3001/24303] Loss: 118452.6996       PSNR: -49.5682  Lr:0.0040
2023-06-25 14:55:02 Train Epoch: 5 [4001/24303] Loss: 7547027.9597      PSNR: -47.2273  Lr:0.0040
2023-06-25 15:23:37 Train Epoch: 5 [5001/24303] Loss: 21878.4844        PSNR: -43.5840  Lr:0.0040
2023-06-25 15:52:12 Train Epoch: 5 [6001/24303] Loss: 14865.7143        PSNR: -39.8757  Lr:0.0040
syn: 0.2692685127258301, out: -3023.16796875, gt: 0.27039244771003723
syn: 0.27017509937286377, out: -3920.54150390625, gt: 0.2701771557331085
syn: 0.3787694275379181, out: -6418.58642578125, gt: 0.380919486284256
syn: 0.4813161790370941, out: -9118.2216796875, gt: 0.48426637053489685
syn: 0.32571378350257874, out: -5087.11181640625, gt: 0.3270050585269928
syn: 0.37952107191085815, out: -4813.4638671875, gt: 0.3808598518371582
syn: 0.35496553778648376, out: -8207.466796875, gt: 0.3556583821773529
syn: 0.3568878173828125, out: -8279.310546875, gt: 0.35667040944099426
syn: 0.39295095205307007, out: -8794.4052734375, gt: 0.39359113574028015
2023-06-25 19:50:28 Train Epoch: 5 [7001/24303] Loss: 317776302100.2708 PSNR: -77.8006  Lr:0.0040
2023-06-25 20:19:04 Train Epoch: 5 [8001/24303] Loss: 8332903.1949      PSNR: -67.9117  Lr:0.0040
2023-06-25 20:47:38 Train Epoch: 5 [9001/24303] Loss: 3297244.6167      PSNR: -64.2780  Lr:0.0040
2023-06-25 21:16:11 Train Epoch: 5 [10001/24303]        Loss: 1155061.6614      PSNR: -60.3232  Lr:0.0040
2023-06-25 21:44:43 Train Epoch: 5 [11001/24303]        Loss: 1047522.0480      PSNR: -57.7211  Lr:0.0040
2023-06-25 22:13:15 Train Epoch: 5 [12001/24303]        Loss: 2484039.2429      PSNR: -57.5243  Lr:0.0040
2023-06-25 22:41:48 Train Epoch: 5 [13001/24303]        Loss: 3951584.3985      PSNR: -50.6481  Lr:0.0040
2023-06-25 23:10:20 Train Epoch: 5 [14001/24303]        Loss: 47034.2792        PSNR: -48.4625  Lr:0.0040
2023-06-25 23:38:53 Train Epoch: 5 [15001/24303]        Loss: 15618.8080        PSNR: -41.3810  Lr:0.0040
2023-06-26 00:07:27 Train Epoch: 5 [16001/24303]        Loss: 7174.2230 PSNR: -37.1039  Lr:0.0040
2023-06-26 00:36:00 Train Epoch: 5 [17001/24303]        Loss: 2961.1377 PSNR: -31.8065  Lr:0.0040
2023-06-26 01:04:33 Train Epoch: 5 [18001/24303]        Loss: 1154.5482 PSNR: -26.3495  Lr:0.0040
2023-06-26 01:33:08 Train Epoch: 5 [19001/24303]        Loss: 384.0601  PSNR: -15.5504  Lr:0.0040
2023-06-26 02:01:43 Train Epoch: 5 [20001/24303]        Loss: 10015731.2603     PSNR: -40.2448  Lr:0.0040
2023-06-26 02:30:18 Train Epoch: 5 [21001/24303]        Loss: 5163.9224 PSNR: -34.8397  Lr:0.0040
2023-06-26 02:58:55 Train Epoch: 5 [22001/24303]        Loss: 1645.2258 PSNR: -30.6750  Lr:0.0040
2023-06-26 03:27:30 Train Epoch: 5 [23001/24303]        Loss: 812.8015  PSNR: -24.4014  Lr:0.0040
2023-06-26 03:56:04 Train Epoch: 5 [24001/24303]        Loss: 129.7493  PSNR: -18.1199  Lr:0.0040
Validation for epoch = 5
(59375s) Epoch [5/5], Val_PSNR:-19.44, Val_SSIM:0.9716

========= Training Complete =========

>>>>>>>>>>>>>>>>>>>>> Complete <<<<<<<<<<<<<<<<<<<<<<<<

{230627 보간된 Flow + 일반 와핑 방식으로 모델 학습}
{Train Dataloader Stride 2 주고, UNet 구조 간소화 수행}
python train.py --data_root /home/work/VFIT/data/vimeo_septuplet/ --log_iter 1000 --epochs 2 --batch_size 6 --test_batch_size 6 --checkpoint_epoch 1

>>>>>> RAFT + MiDaS Guide Video Frame Interpolation <<<<<

Config: Namespace(batch_size=6, checkpoint_epoch=1, checkpoint_root='./checkpoint/', crop_size=(512, 960), cuda=True, data_root='/home/work/VFIT/data/vimeo_septuplet/', depth_model='DPT_Large', device='cuda', epochs=2, flow_model='raft_large', gpu=0, load_from='checkpoint/model_best.pth', log_iter=1000, loss='1*l1+0.5*Huber', num_workers=4, out_root='./output/temp/', seed=1123, softsplat=True, tensorboard_root='./tensorboard/', test_batch_size=6, use_gpu=True)

>>>>>>>>>>>>>>>>>>>>> Initialize <<<<<<<<<<<<<<<<<<<<<<<<
train RAFT + MiDaS Guide VFI DataLoader: 145818 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.407
valid RAFT + MiDaS Guide VFI DataLoader: 18518 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.473
CUDA available True,  Usuable devices: 1,   Current device: 0,  name: NVIDIA A100-PCIE-40GB

raft_large softsplat #params 5257536
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
DPT_Large #params 344055465
Refine Net #params 7990595 

1.000 * l1
0.500 * Huber

>>>>>>>>>>>>>>>>>>> Train & Valid <<<<<<<<<<<<<<<<<<<<<<
2023-06-27 06:02:53 Train Epoch: 1 [1/24303]    Loss: 0.5090    PSNR: 7.2067  Lr:0.0010
2023-06-27 06:25:20 Train Epoch: 1 [1001/24303] Loss: 0.0302    PSNR: 26.2539  Lr:0.0010
2023-06-27 06:47:49 Train Epoch: 1 [2001/24303] Loss: 0.0254    PSNR: 29.4245  Lr:0.0010
2023-06-27 07:10:17 Train Epoch: 1 [3001/24303] Loss: 0.0243    PSNR: 31.1275  Lr:0.0010
2023-06-27 07:32:43 Train Epoch: 1 [4001/24303] Loss: 0.0238    PSNR: 27.4260  Lr:0.0010
2023-06-27 07:55:09 Train Epoch: 1 [5001/24303] Loss: 0.0232    PSNR: 28.3281  Lr:0.0010
2023-06-27 08:17:35 Train Epoch: 1 [6001/24303] Loss: 0.0231    PSNR: 28.6135  Lr:0.0010
2023-06-27 08:40:01 Train Epoch: 1 [7001/24303] Loss: 0.0230    PSNR: 29.5088  Lr:0.0010
2023-06-27 09:02:26 Train Epoch: 1 [8001/24303] Loss: 0.0223    PSNR: 29.5494  Lr:0.0010
2023-06-27 09:24:52 Train Epoch: 1 [9001/24303] Loss: 0.0228    PSNR: 23.1546  Lr:0.0010
2023-06-27 09:47:18 Train Epoch: 1 [10001/24303]        Loss: 0.0221    PSNR: 24.0771  Lr:0.0010
2023-06-27 10:09:44 Train Epoch: 1 [11001/24303]        Loss: 0.0221    PSNR: 28.6451  Lr:0.0010
2023-06-27 10:32:10 Train Epoch: 1 [12001/24303]        Loss: 0.0218    PSNR: 29.8155  Lr:0.0010
2023-06-27 10:54:36 Train Epoch: 1 [13001/24303]        Loss: 0.0219    PSNR: 28.7430  Lr:0.0010
2023-06-27 11:17:02 Train Epoch: 1 [14001/24303]        Loss: 0.0216    PSNR: 26.8620  Lr:0.0010
2023-06-27 11:39:28 Train Epoch: 1 [15001/24303]        Loss: 0.0215    PSNR: 26.9669  Lr:0.0010
2023-06-27 12:01:55 Train Epoch: 1 [16001/24303]        Loss: 0.0215    PSNR: 30.0132  Lr:0.0010
2023-06-27 12:24:22 Train Epoch: 1 [17001/24303]        Loss: 0.0214    PSNR: 30.0894  Lr:0.0010
2023-06-27 12:46:49 Train Epoch: 1 [18001/24303]        Loss: 0.0213    PSNR: 26.7102  Lr:0.0010
2023-06-27 13:09:16 Train Epoch: 1 [19001/24303]        Loss: 0.0216    PSNR: 25.0205  Lr:0.0010
syn: 0.4129442274570465, out: 0.4021449089050293, gt: 0.41558679938316345
2023-06-27 13:32:04 Train Epoch: 1 [20001/24303]        Loss: 0.0213    PSNR: 27.3601  Lr:0.0010
syn: 0.36330264806747437, out: 0.35816457867622375, gt: 0.3628791570663452
syn: 0.33804911375045776, out: 0.34224581718444824, gt: 0.3365810811519623
2023-06-27 13:56:52 Train Epoch: 1 [21001/24303]        Loss: 0.0213    PSNR: 28.5759  Lr:0.0010
2023-06-27 14:19:17 Train Epoch: 1 [22001/24303]        Loss: 0.0211    PSNR: 25.7029  Lr:0.0010
2023-06-27 14:41:42 Train Epoch: 1 [23001/24303]        Loss: 0.0210    PSNR: 25.7914  Lr:0.0010
2023-06-27 15:04:06 Train Epoch: 1 [24001/24303]        Loss: 0.0212    PSNR: 29.0783  Lr:0.0010
Validation for epoch = 1
(36758s) Epoch [1/2], Val_PSNR:28.12, Val_SSIM:0.9998
2023-06-27 16:15:02 Train Epoch: 2 [1/24303]    Loss: 0.0200    PSNR: 27.1575  Lr:0.0005
2023-06-27 16:37:29 Train Epoch: 2 [1001/24303] Loss: 0.0202    PSNR: 28.9721  Lr:0.0005
2023-06-27 16:59:56 Train Epoch: 2 [2001/24303] Loss: 0.0201    PSNR: 26.7532  Lr:0.0005
2023-06-27 17:22:22 Train Epoch: 2 [3001/24303] Loss: 0.0202    PSNR: 28.3077  Lr:0.0005
2023-06-27 17:44:49 Train Epoch: 2 [4001/24303] Loss: 0.0198    PSNR: 25.2879  Lr:0.0005
2023-06-27 18:07:15 Train Epoch: 2 [5001/24303] Loss: 0.0201    PSNR: 30.4768  Lr:0.0005
2023-06-27 18:29:41 Train Epoch: 2 [6001/24303] Loss: 0.0200    PSNR: 30.4497  Lr:0.0005
2023-06-27 18:52:07 Train Epoch: 2 [7001/24303] Loss: 0.0200    PSNR: 24.8674  Lr:0.0005
2023-06-27 19:14:34 Train Epoch: 2 [8001/24303] Loss: 0.0203    PSNR: 30.7132  Lr:0.0005
2023-06-27 19:37:01 Train Epoch: 2 [9001/24303] Loss: 0.0198    PSNR: 32.4084  Lr:0.0005
2023-06-27 19:59:26 Train Epoch: 2 [10001/24303]        Loss: 0.0198    PSNR: 27.2539  Lr:0.0005
2023-06-27 20:21:53 Train Epoch: 2 [11001/24303]        Loss: 0.0200    PSNR: 28.5694  Lr:0.0005
2023-06-27 20:44:19 Train Epoch: 2 [12001/24303]        Loss: 0.0200    PSNR: 25.6936  Lr:0.0005
2023-06-27 21:06:45 Train Epoch: 2 [13001/24303]        Loss: 0.0200    PSNR: 30.6742  Lr:0.0005
2023-06-27 21:29:12 Train Epoch: 2 [14001/24303]        Loss: 0.0200    PSNR: 28.5081  Lr:0.0005
2023-06-27 21:51:38 Train Epoch: 2 [15001/24303]        Loss: 0.0199    PSNR: 31.4458  Lr:0.0005
2023-06-27 22:14:03 Train Epoch: 2 [16001/24303]        Loss: 0.0198    PSNR: 27.8887  Lr:0.0005
2023-06-27 22:36:28 Train Epoch: 2 [17001/24303]        Loss: 0.0199    PSNR: 32.5702  Lr:0.0005
2023-06-27 22:58:54 Train Epoch: 2 [18001/24303]        Loss: 0.0197    PSNR: 29.7204  Lr:0.0005
2023-06-27 23:21:19 Train Epoch: 2 [19001/24303]        Loss: 0.0197    PSNR: 27.1211  Lr:0.0005
syn: 0.3998318612575531, out: 0.3983435332775116, gt: 0.39547938108444214
2023-06-27 23:45:47 Train Epoch: 2 [20001/24303]        Loss: 0.0196    PSNR: 25.9372  Lr:0.0005
syn: 0.3165646493434906, out: 0.31764182448387146, gt: 0.3157906234264374
syn: 0.342769056558609, out: 0.3460663855075836, gt: 0.3415544927120209
2023-06-28 00:14:02 Train Epoch: 2 [21001/24303]        Loss: 0.0199    PSNR: 29.9257  Lr:0.0005
2023-06-28 00:36:27 Train Epoch: 2 [22001/24303]        Loss: 0.0198    PSNR: 35.6448  Lr:0.0005
2023-06-28 00:58:52 Train Epoch: 2 [23001/24303]        Loss: 0.0198    PSNR: 28.9076  Lr:0.0005
2023-06-28 01:21:17 Train Epoch: 2 [24001/24303]        Loss: 0.0196    PSNR: 29.9911  Lr:0.0005
Validation for epoch = 2
(37022s) Epoch [2/2], Val_PSNR:28.50, Val_SSIM:0.9998

========= Training Complete =========

>>>>>>>>>>>>>>>>>>>>> Complete <<<<<<<<<<<<<<<<<<<<<<<<

{230628 보간된 Flow + Softsplat 방식으로 모델 학습}
{Train Dataloader Stride 2 주고, UNet 구조 간소화 수행}
python train.py --data_root /home/work/VFIT/data/vimeo_septuplet/ --out_root ./output/test/ --softsplat True

>>>>>> RAFT + MiDaS Guide Video Frame Interpolation <<<<<

Config: Namespace(batch_size=6, checkpoint_epoch=1, checkpoint_root='./checkpoint/', crop_size=(512, 960), cuda=True, data_root='/home/work/VFIT/data/vimeo_septuplet/', depth_model='DPT_Large', device='cuda', epochs=2, flow_model='raft_large', gpu=0, load_from='checkpoint/model_best.pth', log_iter=1000, loss='1*l1', num_workers=4, out_root='./output/test/', seed=1123, softsplat=True, tensorboard_root='./tensorboard/', test_batch_size=6, use_gpu=True)

>>>>>>>>>>>>>>>>>>>>> Initialize <<<<<<<<<<<<<<<<<<<<<<<<
train RAFT + MiDaS Guide VFI DataLoader: 145818 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.407
valid RAFT + MiDaS Guide VFI DataLoader: 18518 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.473
CUDA available True,  Usuable devices: 1,   Current device: 0,  name: NVIDIA A100-PCIE-40GB

raft_large softsplat #params 5257536
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
DPT_Large #params 344055465
Refine Net #params 7990595 

1.000 * l1

>>>>>>>>>>>>>>>>>>> Train & Valid <<<<<<<<<<<<<<<<<<<<<<
2023-06-28 06:17:04 Train Epoch: 1 [1/24303]    Loss: 0.4431    PSNR: 7.2067  Lr:0.0020
2023-06-28 06:39:28 Train Epoch: 1 [1001/24303] Loss: 0.0316    PSNR: 25.7097  Lr:0.0020
2023-06-28 07:01:53 Train Epoch: 1 [2001/24303] Loss: 0.0270    PSNR: 25.0253  Lr:0.0020
2023-06-28 07:24:18 Train Epoch: 1 [3001/24303] Loss: 0.0248    PSNR: 30.7059  Lr:0.0020
2023-06-28 07:46:43 Train Epoch: 1 [4001/24303] Loss: 0.0241    PSNR: 27.1578  Lr:0.0020
2023-06-28 08:09:08 Train Epoch: 1 [5001/24303] Loss: 0.0232    PSNR: 28.3835  Lr:0.0010
2023-06-28 08:31:32 Train Epoch: 1 [6001/24303] Loss: 0.0217    PSNR: 28.6463  Lr:0.0010
2023-06-28 08:53:57 Train Epoch: 1 [7001/24303] Loss: 0.0216    PSNR: 29.6315  Lr:0.0010
2023-06-28 09:16:21 Train Epoch: 1 [8001/24303] Loss: 0.0211    PSNR: 29.5718  Lr:0.0010
2023-06-28 09:38:45 Train Epoch: 1 [9001/24303] Loss: 0.0214    PSNR: 23.0577  Lr:0.0010
2023-06-28 10:01:09 Train Epoch: 1 [10001/24303]        Loss: 0.0210    PSNR: 23.8524  Lr:0.0005
2023-06-28 10:23:33 Train Epoch: 1 [11001/24303]        Loss: 0.0206    PSNR: 29.8042  Lr:0.0005
2023-06-28 10:45:57 Train Epoch: 1 [12001/24303]        Loss: 0.0204    PSNR: 29.6801  Lr:0.0005
2023-06-28 11:08:22 Train Epoch: 1 [13001/24303]        Loss: 0.0205    PSNR: 28.8172  Lr:0.0005
2023-06-28 11:30:45 Train Epoch: 1 [14001/24303]        Loss: 0.0203    PSNR: 26.5385  Lr:0.0005
2023-06-28 11:53:10 Train Epoch: 1 [15001/24303]        Loss: 0.0203    PSNR: 26.8496  Lr:0.0003
2023-06-28 12:15:34 Train Epoch: 1 [16001/24303]        Loss: 0.0200    PSNR: 30.2326  Lr:0.0003
2023-06-28 12:37:58 Train Epoch: 1 [17001/24303]        Loss: 0.0200    PSNR: 30.3561  Lr:0.0003
2023-06-28 13:00:22 Train Epoch: 1 [18001/24303]        Loss: 0.0199    PSNR: 26.6159  Lr:0.0003
2023-06-28 13:22:47 Train Epoch: 1 [19001/24303]        Loss: 0.0202    PSNR: 24.7703  Lr:0.0003
syn: 0.4132135808467865, out: 0.4144348204135895, gt: 0.41558679938316345
2023-06-28 13:45:32 Train Epoch: 1 [20001/24303]        Loss: 0.0199    PSNR: 27.8102  Lr:0.0001
syn: 0.3632951080799103, out: 0.36340558528900146, gt: 0.3628791570663452
syn: 0.3380320072174072, out: 0.3374401032924652, gt: 0.3365810811519623
2023-06-28 14:10:23 Train Epoch: 1 [21001/24303]        Loss: 0.0196    PSNR: 28.5049  Lr:0.0001
2023-06-28 14:32:49 Train Epoch: 1 [22001/24303]        Loss: 0.0197    PSNR: 25.6702  Lr:0.0001
2023-06-28 14:55:14 Train Epoch: 1 [23001/24303]        Loss: 0.0197    PSNR: 25.8103  Lr:0.0001
2023-06-28 15:17:40 Train Epoch: 1 [24001/24303]        Loss: 0.0199    PSNR: 29.0079  Lr:0.0001
Validation for epoch = 1
(36727s) Epoch [1/2], Val_PSNR:28.14, Val_SSIM:0.9998
2023-06-28 16:28:43 Train Epoch: 2 [1/24303]    Loss: 0.0182    PSNR: 27.0731  Lr:0.0001
2023-06-28 16:51:11 Train Epoch: 2 [1001/24303] Loss: 0.0197    PSNR: 28.8811  Lr:0.0001
2023-06-28 17:13:39 Train Epoch: 2 [2001/24303] Loss: 0.0196    PSNR: 26.6573  Lr:0.0001
2023-06-28 17:36:06 Train Epoch: 2 [3001/24303] Loss: 0.0197    PSNR: 27.9796  Lr:0.0001
2023-06-28 17:58:33 Train Epoch: 2 [4001/24303] Loss: 0.0194    PSNR: 24.8694  Lr:0.0001
2023-06-28 18:20:59 Train Epoch: 2 [5001/24303] Loss: 0.0197    PSNR: 30.1880  Lr:0.0001
2023-06-28 18:43:27 Train Epoch: 2 [6001/24303] Loss: 0.0196    PSNR: 30.3235  Lr:0.0001
2023-06-28 19:05:54 Train Epoch: 2 [7001/24303] Loss: 0.0195    PSNR: 24.7287  Lr:0.0001
2023-06-28 19:28:22 Train Epoch: 2 [8001/24303] Loss: 0.0198    PSNR: 30.5924  Lr:0.0001
2023-06-28 19:50:47 Train Epoch: 2 [9001/24303] Loss: 0.0194    PSNR: 32.1085  Lr:0.0001
2023-06-28 20:13:15 Train Epoch: 2 [10001/24303]        Loss: 0.0193    PSNR: 26.8380  Lr:0.0001
2023-06-28 20:35:43 Train Epoch: 2 [11001/24303]        Loss: 0.0196    PSNR: 28.3769  Lr:0.0001
2023-06-28 20:58:09 Train Epoch: 2 [12001/24303]        Loss: 0.0195    PSNR: 25.6137  Lr:0.0001
2023-06-28 21:20:37 Train Epoch: 2 [13001/24303]        Loss: 0.0196    PSNR: 30.4557  Lr:0.0001
2023-06-28 21:43:04 Train Epoch: 2 [14001/24303]        Loss: 0.0196    PSNR: 28.2777  Lr:0.0001
2023-06-28 22:05:33 Train Epoch: 2 [15001/24303]        Loss: 0.0195    PSNR: 31.3489  Lr:0.0001
2023-06-28 22:27:59 Train Epoch: 2 [16001/24303]        Loss: 0.0194    PSNR: 27.4782  Lr:0.0000
2023-06-28 22:50:27 Train Epoch: 2 [17001/24303]        Loss: 0.0194    PSNR: 32.2145  Lr:0.0000
2023-06-28 23:12:54 Train Epoch: 2 [18001/24303]        Loss: 0.0192    PSNR: 29.5030  Lr:0.0000
2023-06-28 23:35:20 Train Epoch: 2 [19001/24303]        Loss: 0.0193    PSNR: 26.8333  Lr:0.0000
syn: 0.39987611770629883, out: 0.39957112073898315, gt: 0.39547938108444214
2023-06-28 23:59:56 Train Epoch: 2 [20001/24303]        Loss: 0.0192    PSNR: 25.4815  Lr:0.0000
syn: 0.3165671229362488, out: 0.31594809889793396, gt: 0.3157906234264374
syn: 0.34287145733833313, out: 0.3432740867137909, gt: 0.3415544927120209
2023-06-29 00:28:25 Train Epoch: 2 [21001/24303]        Loss: 0.0195    PSNR: 29.6853  Lr:0.0000
2023-06-29 00:50:51 Train Epoch: 2 [22001/24303]        Loss: 0.0194    PSNR: 35.5782  Lr:0.0000
2023-06-29 01:13:17 Train Epoch: 2 [23001/24303]        Loss: 0.0194    PSNR: 28.9919  Lr:0.0000
2023-06-29 01:35:39 Train Epoch: 2 [24001/24303]        Loss: 0.0192    PSNR: 29.8657  Lr:0.0000
Validation for epoch = 2
(37077s) Epoch [2/2], Val_PSNR:28.27, Val_SSIM:0.9998

========= Training Complete =========

>>>>>>>>>>>>>>>>>>>>> Complete <<<<<<<<<<<<<<<<<<<<<<<<

{230628 보간된 Flow + Softsplat 방식으로 모델 학습}
{Train Dataloader Stride 2 주고, UNet 구조 간소화 수행}
>>>>>> RAFT + MiDaS Guide Video Frame Interpolation <<<<<

Config: Namespace(batch_size=6, checkpoint_epoch=1, checkpoint_root='./checkpoint/', crop_size=(512, 960), cuda=True, data_root='/home/work/VFIT/data/vimeo_septuplet/', depth_model='DPT_Large', device='cuda', epochs=5, flow_model='raft_large', gpu=0, load_from='checkpoint/model_best.pth', log_iter=1000, loss='1*l1', num_workers=4, out_root='./output/temp/', seed=1123, softsplat=True, tensorboard_root='./tensorboard/', test_batch_size=6, use_gpu=True)

>>>>>>>>>>>>>>>>>>>>> Initialize <<<<<<<<<<<<<<<<<<<<<<<<
train RAFT + MiDaS Guide VFI DataLoader: 145818 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.407
valid RAFT + MiDaS Guide VFI DataLoader: 18518 batch,  input 2 frames output 1 frames
input: 2 x torch.Size([6, 3, 512, 960]),  gt: torch.Size([6, 3, 512, 960]),  torch.float32,  0.473
CUDA available True,  Usuable devices: 1,   Current device: 0,  name: NVIDIA A100-PCIE-40GB

raft_large softsplat #params 5257536
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
Using cache found in /home/work/.cache/torch/hub/intel-isl_MiDaS_master
DPT_Large #params 344055465
Refine Net #params 7990595 

1.000 * l1

>>>>>>>>>>>>>>>>>>> Train & Valid <<<<<<<<<<<<<<<<<<<<<<
2023-06-29 07:19:48 Train Epoch: 1 [1/24303]    Loss: 0.4431    PSNR: 7.2067  Lr:0.0020
2023-06-29 07:42:11 Train Epoch: 1 [1001/24303] Loss: 0.0321    PSNR: 25.4745  Lr:0.0020
2023-06-29 08:04:35 Train Epoch: 1 [2001/24303] Loss: 0.0264    PSNR: 28.2931  Lr:0.0020
2023-06-29 08:26:58 Train Epoch: 1 [3001/24303] Loss: 0.0248    PSNR: 30.7912  Lr:0.0020
2023-06-29 08:49:22 Train Epoch: 1 [4001/24303] Loss: 0.0243    PSNR: 27.1640  Lr:0.0020
2023-06-29 09:11:46 Train Epoch: 1 [5001/24303] Loss: 0.0233    PSNR: 27.4190  Lr:0.0010
2023-06-29 09:34:09 Train Epoch: 1 [6001/24303] Loss: 0.0216    PSNR: 28.6841  Lr:0.0010
2023-06-29 09:56:31 Train Epoch: 1 [7001/24303] Loss: 0.0215    PSNR: 29.6842  Lr:0.0010
2023-06-29 10:18:53 Train Epoch: 1 [8001/24303] Loss: 0.0211    PSNR: 29.7575  Lr:0.0010
2023-06-29 10:41:17 Train Epoch: 1 [9001/24303] Loss: 0.0215    PSNR: 23.0472  Lr:0.0010
2023-06-29 11:03:41 Train Epoch: 1 [10001/24303]        Loss: 0.0211    PSNR: 23.7722  Lr:0.0005
2023-06-29 11:26:05 Train Epoch: 1 [11001/24303]        Loss: 0.0207    PSNR: 29.6762  Lr:0.0005
2023-06-29 11:48:29 Train Epoch: 1 [12001/24303]        Loss: 0.0204    PSNR: 29.7355  Lr:0.0005
2023-06-29 12:10:53 Train Epoch: 1 [13001/24303]        Loss: 0.0205    PSNR: 28.7614  Lr:0.0005
2023-06-29 12:33:16 Train Epoch: 1 [14001/24303]        Loss: 0.0203    PSNR: 26.5671  Lr:0.0005
2023-06-29 12:55:39 Train Epoch: 1 [15001/24303]        Loss: 0.0204    PSNR: 26.9130  Lr:0.0003
2023-06-29 13:18:03 Train Epoch: 1 [16001/24303]        Loss: 0.0200    PSNR: 30.2008  Lr:0.0003
2023-06-29 13:40:26 Train Epoch: 1 [17001/24303]        Loss: 0.0201    PSNR: 30.3216  Lr:0.0003
2023-06-29 14:02:50 Train Epoch: 1 [18001/24303]        Loss: 0.0199    PSNR: 26.5967  Lr:0.0003
2023-06-29 14:25:13 Train Epoch: 1 [19001/24303]        Loss: 0.0203    PSNR: 24.8335  Lr:0.0003
syn: 0.41321325302124023, out: 0.4132939577102661, gt: 0.41558679938316345
2023-06-29 14:47:57 Train Epoch: 1 [20001/24303]        Loss: 0.0199    PSNR: 27.7225  Lr:0.0001
syn: 0.3632949888706207, out: 0.36304208636283875, gt: 0.3628791570663452
syn: 0.3380316197872162, out: 0.3377012610435486, gt: 0.3365810811519623
2023-06-29 15:12:46 Train Epoch: 1 [21001/24303]        Loss: 0.0196    PSNR: 28.4418  Lr:0.0001
2023-06-29 15:35:09 Train Epoch: 1 [22001/24303]        Loss: 0.0197    PSNR: 25.6451  Lr:0.0001
2023-06-29 15:57:32 Train Epoch: 1 [23001/24303]        Loss: 0.0197    PSNR: 25.8119  Lr:0.0001
2023-06-29 16:19:56 Train Epoch: 1 [24001/24303]        Loss: 0.0199    PSNR: 28.9579  Lr:0.0001
Validation for epoch = 1
(36682s) Epoch [1/5], Val_PSNR:28.15, Val_SSIM:0.9998
2023-06-29 17:30:44 Train Epoch: 2 [1/24303]    Loss: 0.0182    PSNR: 27.1162  Lr:0.0001
2023-06-29 17:53:11 Train Epoch: 2 [1001/24303] Loss: 0.0197    PSNR: 28.8008  Lr:0.0001
2023-06-29 18:15:39 Train Epoch: 2 [2001/24303] Loss: 0.0196    PSNR: 26.6148  Lr:0.0001
2023-06-29 18:38:06 Train Epoch: 2 [3001/24303] Loss: 0.0198    PSNR: 27.9428  Lr:0.0001
2023-06-29 19:00:33 Train Epoch: 2 [4001/24303] Loss: 0.0194    PSNR: 25.0024  Lr:0.0001
2023-06-29 19:22:59 Train Epoch: 2 [5001/24303] Loss: 0.0197    PSNR: 30.4085  Lr:0.0001
2023-06-29 19:45:26 Train Epoch: 2 [6001/24303] Loss: 0.0196    PSNR: 30.2930  Lr:0.0001
2023-06-29 20:07:52 Train Epoch: 2 [7001/24303] Loss: 0.0195    PSNR: 24.7165  Lr:0.0001
2023-06-29 20:30:18 Train Epoch: 2 [8001/24303] Loss: 0.0198    PSNR: 30.5404  Lr:0.0001
2023-06-29 20:52:45 Train Epoch: 2 [9001/24303] Loss: 0.0194    PSNR: 31.9937  Lr:0.0001
2023-06-29 21:15:13 Train Epoch: 2 [10001/24303]        Loss: 0.0193    PSNR: 26.8017  Lr:0.0001
2023-06-29 21:37:41 Train Epoch: 2 [11001/24303]        Loss: 0.0196    PSNR: 28.3910  Lr:0.0001
2023-06-29 22:00:07 Train Epoch: 2 [12001/24303]        Loss: 0.0195    PSNR: 25.6251  Lr:0.0001
2023-06-29 22:22:34 Train Epoch: 2 [13001/24303]        Loss: 0.0196    PSNR: 30.4620  Lr:0.0001
2023-06-29 22:45:01 Train Epoch: 2 [14001/24303]        Loss: 0.0196    PSNR: 28.3233  Lr:0.0001
2023-06-29 23:07:28 Train Epoch: 2 [15001/24303]        Loss: 0.0195    PSNR: 31.3239  Lr:0.0001
2023-06-29 23:29:54 Train Epoch: 2 [16001/24303]        Loss: 0.0194    PSNR: 27.5978  Lr:0.0000
2023-06-29 23:52:19 Train Epoch: 2 [17001/24303]        Loss: 0.0194    PSNR: 32.2127  Lr:0.0000
2023-06-30 00:14:46 Train Epoch: 2 [18001/24303]        Loss: 0.0192    PSNR: 29.5118  Lr:0.0000
2023-06-30 00:37:12 Train Epoch: 2 [19001/24303]        Loss: 0.0193    PSNR: 26.8644  Lr:0.0000
syn: 0.39987584948539734, out: 0.3997550904750824, gt: 0.39547938108444214
2023-06-30 01:01:46 Train Epoch: 2 [20001/24303]        Loss: 0.0192    PSNR: 25.4370  Lr:0.0000
syn: 0.3165666460990906, out: 0.31584545969963074, gt: 0.3157906234264374
syn: 0.34287142753601074, out: 0.3431621491909027, gt: 0.3415544927120209
2023-06-30 01:30:15 Train Epoch: 2 [21001/24303]        Loss: 0.0195    PSNR: 29.5318  Lr:0.0000
2023-06-30 01:52:40 Train Epoch: 2 [22001/24303]        Loss: 0.0194    PSNR: 35.5871  Lr:0.0000
2023-06-30 02:15:05 Train Epoch: 2 [23001/24303]        Loss: 0.0194    PSNR: 28.9475  Lr:0.0000
2023-06-30 02:37:30 Train Epoch: 2 [24001/24303]        Loss: 0.0192    PSNR: 29.8508  Lr:0.0000
Validation for epoch = 2
(37064s) Epoch [2/5], Val_PSNR:28.28, Val_SSIM:0.9998
2023-06-30 03:48:28 Train Epoch: 3 [1/24303]    Loss: 0.0151    PSNR: 30.2423  Lr:0.0000
2023-06-30 04:10:52 Train Epoch: 3 [1001/24303] Loss: 0.0192    PSNR: 29.1340  Lr:0.0000
2023-06-30 04:33:15 Train Epoch: 3 [2001/24303] Loss: 0.0194    PSNR: 25.9808  Lr:0.0000
2023-06-30 04:55:39 Train Epoch: 3 [3001/24303] Loss: 0.0191    PSNR: 28.4229  Lr:0.0000
2023-06-30 05:18:04 Train Epoch: 3 [4001/24303] Loss: 0.0194    PSNR: 27.8847  Lr:0.0000
2023-06-30 05:40:28 Train Epoch: 3 [5001/24303] Loss: 0.0195    PSNR: 26.2264  Lr:0.0000
2023-06-30 06:02:52 Train Epoch: 3 [6001/24303] Loss: 0.0192    PSNR: 27.8156  Lr:0.0000
2023-06-30 06:25:16 Train Epoch: 3 [7001/24303] Loss: 0.0193    PSNR: 28.4003  Lr:0.0000
2023-06-30 06:47:41 Train Epoch: 3 [8001/24303] Loss: 0.0195    PSNR: 27.8507  Lr:0.0000
2023-06-30 07:10:06 Train Epoch: 3 [9001/24303] Loss: 0.0196    PSNR: 30.0694  Lr:0.0000
2023-06-30 07:32:31 Train Epoch: 3 [10001/24303]        Loss: 0.0193    PSNR: 26.8052  Lr:0.0000
2023-06-30 07:54:54 Train Epoch: 3 [11001/24303]        Loss: 0.0192    PSNR: 28.2698  Lr:0.0000
2023-06-30 08:17:18 Train Epoch: 3 [12001/24303]        Loss: 0.0194    PSNR: 26.7406  Lr:0.0000
2023-06-30 08:39:42 Train Epoch: 3 [13001/24303]        Loss: 0.0193    PSNR: 25.0871  Lr:0.0000
2023-06-30 09:02:05 Train Epoch: 3 [14001/24303]        Loss: 0.0192    PSNR: 29.1617  Lr:0.0000
2023-06-30 09:24:29 Train Epoch: 3 [15001/24303]        Loss: 0.0192    PSNR: 27.1181  Lr:0.0000
2023-06-30 09:46:50 Train Epoch: 3 [16001/24303]        Loss: 0.0191    PSNR: 26.1285  Lr:0.0000
2023-06-30 10:09:15 Train Epoch: 3 [17001/24303]        Loss: 0.0193    PSNR: 30.6911  Lr:0.0000
2023-06-30 10:31:37 Train Epoch: 3 [18001/24303]        Loss: 0.0192    PSNR: 26.9003  Lr:0.0000
2023-06-30 10:54:01 Train Epoch: 3 [19001/24303]        Loss: 0.0192    PSNR: 28.5780  Lr:0.0000
syn: 0.4182223379611969, out: 0.4182590842247009, gt: 0.42413875460624695
2023-06-30 11:20:15 Train Epoch: 3 [20001/24303]        Loss: 0.0192    PSNR: 22.7754  Lr:0.0000
syn: 0.3287861943244934, out: 0.32885345816612244, gt: 0.32843613624572754
syn: 0.3090112507343292, out: 0.30607348680496216, gt: 0.305851012468338
2023-06-30 11:52:07 Train Epoch: 3 [21001/24303]        Loss: 0.0189    PSNR: 28.8509  Lr:0.0000
2023-06-30 12:14:32 Train Epoch: 3 [22001/24303]        Loss: 0.0192    PSNR: 33.7784  Lr:0.0000
2023-06-30 12:36:55 Train Epoch: 3 [23001/24303]        Loss: 0.0194    PSNR: 26.9419  Lr:0.0000
2023-06-30 12:59:20 Train Epoch: 3 [24001/24303]        Loss: 0.0191    PSNR: 25.5088  Lr:0.0000
Validation for epoch = 3
(37295s) Epoch [3/5], Val_PSNR:28.31, Val_SSIM:0.9998
2023-06-30 14:10:03 Train Epoch: 4 [1/24303]    Loss: 0.0117    PSNR: 32.7133  Lr:0.0000
2023-06-30 14:32:28 Train Epoch: 4 [1001/24303] Loss: 0.0195    PSNR: 29.7788  Lr:0.0000
2023-06-30 14:54:53 Train Epoch: 4 [2001/24303] Loss: 0.0194    PSNR: 31.4257  Lr:0.0000
2023-06-30 15:17:18 Train Epoch: 4 [3001/24303] Loss: 0.0192    PSNR: 25.8249  Lr:0.0000
2023-06-30 15:39:43 Train Epoch: 4 [4001/24303] Loss: 0.0196    PSNR: 26.5671  Lr:0.0000
2023-06-30 16:02:08 Train Epoch: 4 [5001/24303] Loss: 0.0193    PSNR: 29.0409  Lr:0.0000
2023-06-30 16:24:32 Train Epoch: 4 [6001/24303] Loss: 0.0191    PSNR: 26.4684  Lr:0.0000
2023-06-30 16:46:57 Train Epoch: 4 [7001/24303] Loss: 0.0193    PSNR: 27.1951  Lr:0.0000
2023-06-30 17:09:21 Train Epoch: 4 [8001/24303] Loss: 0.0193    PSNR: 29.3356  Lr:0.0000
2023-06-30 17:31:46 Train Epoch: 4 [9001/24303] Loss: 0.0192    PSNR: 32.0098  Lr:0.0000
2023-06-30 17:54:10 Train Epoch: 4 [10001/24303]        Loss: 0.0192    PSNR: 28.4492  Lr:0.0000
2023-06-30 18:16:35 Train Epoch: 4 [11001/24303]        Loss: 0.0193    PSNR: 30.7205  Lr:0.0000
2023-06-30 18:39:00 Train Epoch: 4 [12001/24303]        Loss: 0.0194    PSNR: 27.2093  Lr:0.0000
2023-06-30 19:01:23 Train Epoch: 4 [13001/24303]        Loss: 0.0190    PSNR: 27.7950  Lr:0.0000
2023-06-30 19:23:47 Train Epoch: 4 [14001/24303]        Loss: 0.0194    PSNR: 29.0203  Lr:0.0000
2023-06-30 19:46:12 Train Epoch: 4 [15001/24303]        Loss: 0.0192    PSNR: 28.4344  Lr:0.0000
2023-06-30 20:08:36 Train Epoch: 4 [16001/24303]        Loss: 0.0188    PSNR: 30.6155  Lr:0.0000
2023-06-30 20:31:00 Train Epoch: 4 [17001/24303]        Loss: 0.0191    PSNR: 25.2818  Lr:0.0000
2023-06-30 20:53:24 Train Epoch: 4 [18001/24303]        Loss: 0.0192    PSNR: 30.5932  Lr:0.0000
2023-06-30 21:15:48 Train Epoch: 4 [19001/24303]        Loss: 0.0191    PSNR: 26.4795  Lr:0.0000
syn: 0.3483279049396515, out: 0.347955584526062, gt: 0.3507627546787262
2023-06-30 21:43:45 Train Epoch: 4 [20001/24303]        Loss: 0.0192    PSNR: 27.7170  Lr:0.0000
syn: 0.3902599513530731, out: 0.3913991153240204, gt: 0.39099153876304626
syn: 0.42576298117637634, out: 0.4266433119773865, gt: 0.4260658025741577
2023-06-30 22:19:14 Train Epoch: 4 [21001/24303]        Loss: 0.0192    PSNR: 28.7955  Lr:0.0000
2023-06-30 22:41:38 Train Epoch: 4 [22001/24303]        Loss: 0.0189    PSNR: 30.2949  Lr:0.0000
2023-06-30 23:04:02 Train Epoch: 4 [23001/24303]        Loss: 0.0193    PSNR: 28.6682  Lr:0.0000
2023-06-30 23:26:26 Train Epoch: 4 [24001/24303]        Loss: 0.0191    PSNR: 27.6926  Lr:0.0000
Validation for epoch = 4
(37641s) Epoch [4/5], Val_PSNR:28.33, Val_SSIM:0.9998
2023-07-01 00:37:24 Train Epoch: 5 [1/24303]    Loss: 0.0106    PSNR: 31.7892  Lr:0.0000
2023-07-01 00:59:50 Train Epoch: 5 [1001/24303] Loss: 0.0193    PSNR: 31.3641  Lr:0.0000
2023-07-01 01:22:16 Train Epoch: 5 [2001/24303] Loss: 0.0191    PSNR: 30.1939  Lr:0.0000
2023-07-01 01:44:44 Train Epoch: 5 [3001/24303] Loss: 0.0192    PSNR: 26.8049  Lr:0.0000
2023-07-01 02:07:10 Train Epoch: 5 [4001/24303] Loss: 0.0192    PSNR: 28.5076  Lr:0.0000
2023-07-01 02:29:37 Train Epoch: 5 [5001/24303] Loss: 0.0192    PSNR: 25.9383  Lr:0.0000
2023-07-01 02:52:03 Train Epoch: 5 [6001/24303] Loss: 0.0193    PSNR: 30.1400  Lr:0.0000
2023-07-01 03:14:29 Train Epoch: 5 [7001/24303] Loss: 0.0192    PSNR: 25.6086  Lr:0.0000
2023-07-01 03:36:56 Train Epoch: 5 [8001/24303] Loss: 0.0192    PSNR: 27.7441  Lr:0.0000
2023-07-01 03:59:22 Train Epoch: 5 [9001/24303] Loss: 0.0190    PSNR: 27.2091  Lr:0.0000
2023-07-01 04:21:47 Train Epoch: 5 [10001/24303]        Loss: 0.0192    PSNR: 28.9351  Lr:0.0000
2023-07-01 04:44:13 Train Epoch: 5 [11001/24303]        Loss: 0.0192    PSNR: 26.7334  Lr:0.0000
2023-07-01 05:06:37 Train Epoch: 5 [12001/24303]        Loss: 0.0192    PSNR: 25.3465  Lr:0.0000
2023-07-01 05:29:02 Train Epoch: 5 [13001/24303]        Loss: 0.0192    PSNR: 26.4896  Lr:0.0000
2023-07-01 05:51:26 Train Epoch: 5 [14001/24303]        Loss: 0.0192    PSNR: 26.9877  Lr:0.0000
2023-07-01 06:13:50 Train Epoch: 5 [15001/24303]        Loss: 0.0193    PSNR: 29.8342  Lr:0.0000
2023-07-01 06:36:14 Train Epoch: 5 [16001/24303]        Loss: 0.0192    PSNR: 28.1386  Lr:0.0000
2023-07-01 06:58:38 Train Epoch: 5 [17001/24303]        Loss: 0.0194    PSNR: 26.7021  Lr:0.0000
2023-07-01 07:21:01 Train Epoch: 5 [18001/24303]        Loss: 0.0188    PSNR: 28.4248  Lr:0.0000
2023-07-01 07:43:24 Train Epoch: 5 [19001/24303]        Loss: 0.0191    PSNR: 30.0542  Lr:0.0000
syn: 0.37626126408576965, out: 0.37637707591056824, gt: 0.37657272815704346
2023-07-01 08:13:18 Train Epoch: 5 [20001/24303]        Loss: 0.0194    PSNR: 31.4062  Lr:0.0000
syn: 0.48050209879875183, out: 0.4819239675998688, gt: 0.48458918929100037
syn: 0.275714248418808, out: 0.27519819140434265, gt: 0.2752396762371063
2023-07-01 08:52:14 Train Epoch: 5 [21001/24303]        Loss: 0.0190    PSNR: 26.0839  Lr:0.0000
2023-07-01 09:14:37 Train Epoch: 5 [22001/24303]        Loss: 0.0192    PSNR: 25.3922  Lr:0.0000
2023-07-01 09:37:01 Train Epoch: 5 [23001/24303]        Loss: 0.0188    PSNR: 28.7985  Lr:0.0000
2023-07-01 09:59:25 Train Epoch: 5 [24001/24303]        Loss: 0.0194    PSNR: 30.4318  Lr:0.0000
Validation for epoch = 5
(37965s) Epoch [5/5], Val_PSNR:28.34, Val_SSIM:0.9998

========= Training Complete =========

>>>>>>>>>>>>>>>>>>>>> Complete <<<<<<<<<<<<<<<<<<<<<<<<

